{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoJCV4gk7SM0sTA3tc56fm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BilawalBaloch/Data-Science-ML/blob/main/concepts_of_ML_ipyn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All CONCEPTs OF ML**\n",
        "\n",
        "Regression, classification, and clustering are fundamental tasks in machine learning.\n",
        "\n",
        "**Regression** is used to predict a continuous target variable. For example, predicting house prices based on features\n",
        " like size and location is a regression problem. Linear regression, polynomial regression, and support vector regression are common regression algorithms.\n",
        "\n",
        "**Classification** is used to predict a categorical target variable. For example, classifying emails as spam or not spam,\n",
        " or identifying whether an image contains a cat or a dog, are classification problems. Logistic regression, decision trees, random forests, and support vector machines are common classification algorithms.\n",
        "\n",
        "**Clustering** is an unsupervised learning technique used to group similar data points together.\n",
        " Unlike regression and classification, there is no target variable in clustering. Examples include grouping customers based on their purchasing behavior or segmenting images based on color. K-means and hierarchical clustering are popular clustering algorithms.\n",
        "\n",
        "**Metrics** are used to evaluate the performance of machine learning models. The choice of metric depends on the task and the specific problem.\n",
        "*   For **regression**, common metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared.\n",
        "*   For **classification**, common metrics include accuracy, precision, recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC).\n",
        "*   For **clustering**, metrics like silhouette score and Davies-Bouldin index can be used, although evaluating clustering is often more subjective.\n",
        "\n",
        "**Cross-validation** is a technique used to assess how well a model generalizes to\n",
        "unseen data and to prevent overfitting. It involves splitting the dataset into multiple folds.\n",
        "The model is trained on a subset of the folds and evaluated on the remaining fold. This process is repeated multiple times,\n",
        " with each fold serving as the evaluation set once. K-fold cross-validation is a common approach.\n",
        "\n",
        "**Overfitting** occurs when a model learns the training data too well, including the noise and outliers.\n",
        " This leads to poor performance on unseen data. An overfit model has high variance.\n",
        "\n",
        "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data.\n",
        "This results in poor performance on both training and unseen data. An underfit model has high bias.\n",
        "\n",
        "**Feature Engineering** is the process of creating new features or transforming existing ones to improve\n",
        "the performance of a machine learning model. This often requires domain knowledge and can involve tasks like creating interaction terms,\n",
        " polynomial features, or extracting information from timestamps or text.\n",
        "\n",
        "**Feature Selection** is the process of choosing a subset of the most relevant features from the original set.\n",
        "This can help to reduce dimensionality, improve model performance, reduce training time, and make the model more interpretable.\n",
        " Techniques include filter methods (based on statistical measures), wrapper methods (using a model's performance to select features),\n",
        "  and embedded methods (where feature selection is part of the model training process)."
      ],
      "metadata": {
        "id": "_AY97oo8iLYr"
      }
    }
  ]
}